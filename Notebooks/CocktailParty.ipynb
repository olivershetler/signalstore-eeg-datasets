{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# To be able to make edits to repo without having to restart notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T01:06:28.569301800Z",
     "start_time": "2024-07-03T01:06:28.461581Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T01:06:34.648162500Z",
     "start_time": "2024-07-03T01:06:32.539148900Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_path = Path(\"D:\\BlcRepo\\OtherCode\\Generative_Neuroscience\\Dataset\\Broderick\\Cocktail Party\")\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from signalstore import UnitOfWorkProvider\n",
    "from mongomock import MongoClient\n",
    "#from pymongo import MongoClient\n",
    "from fsspec.implementations.local import LocalFileSystem\n",
    "from fsspec import get_mapper\n",
    "from fsspec.implementations.dirfs import DirFileSystem\n",
    "from tempfile import TemporaryDirectory\n",
    "import fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cocktail Party Experiment\n",
    "\n",
    "## EEG Files\n",
    "eegData: EEG Data, Time Locked to the onset of the speech stimulus.   \n",
    "Format: Channels (128) x Time Points\n",
    "\n",
    "mastoids: Mastoid Channels, Time Locked to the onset of the speech stimulus. \n",
    "Format: Channels (Left=1 Right=2) x Time Points\n",
    "\n",
    "fs: Sampling Rate \n",
    "\n",
    "EEG data is unfiltered, unreferenced and sampled at 128Hz\n",
    "\n",
    "## Experiment Information\n",
    "Subjects 1-17 were instructed to attend to 'Twenty Thousand Leagues Under the Sea' (20000), played in the left ear\n",
    "Subjects 18-33 were instructed to attend to 'Journey to the Centre of the Earth' (Journey), played in the right ear\n",
    "\n",
    "## Behavioral Data\n",
    "score: Comprehension question scores for attended and unattended stories.\n",
    "Format: Subjects x Run x Story (1=Attended, 2=Unattended)\n",
    "\n",
    "## Stimuli Data Files\n",
    "\n",
    "wordVec = List of all the content words for a given trial\n",
    "onset_time = Onset time of the word in the corresponding cell of 'wordVec' (given in seconds)\n",
    "offset_time = Offset time of the word in the corresponding cell of 'wordVec' (given in seconds)\n",
    "sentence_boundaries = Time of sentence close (in seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T01:06:37.986167600Z",
     "start_time": "2024-07-03T01:06:37.895737600Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_eeg_data(data_path):\n",
    "    eeg_dir = data_path / \"EEG\"\n",
    "    subjects = [sdir for sdir in os.listdir(eeg_dir) if os.path.isdir(eeg_dir / sdir)]\n",
    "    behavior_path = data_path / \"Behavioural Data\" / \"Comprehension Scores.mat\"\n",
    "    behavior_data = sio.loadmat(behavior_path)\n",
    "    behavior_scores = behavior_data['score']\n",
    "    for sub_n, subject in enumerate(subjects):\n",
    "        sub_n += 1\n",
    "        subject_dir = eeg_dir / subject\n",
    "        # list run mat files\n",
    "        runs = os.listdir(subject_dir)\n",
    "        for run_n, run in enumerate(runs):\n",
    "            run_n += 1\n",
    "            mfile = sio.loadmat(subject_dir / run)\n",
    "            eeg_data = np.array(mfile['eegData'])\n",
    "            # print(eeg_data.shape)\n",
    "            mastoid_data = mfile['mastoids']\n",
    "            fs = int(mfile['fs'][0][0])\n",
    "            max_time = eeg_data.shape[0] / fs\n",
    "            time_steps = np.linspace(0, max_time, eeg_data.shape[0]).squeeze()\n",
    "            if sub_n <= 17:\n",
    "                attending_direction = 'left'\n",
    "                attending_story = 'Twenty Thousand Leagues Under the Sea'\n",
    "            else:\n",
    "                attending_direction = 'right'\n",
    "                attending_story = 'Journey to the Center of the Earth'\n",
    "            behavior_score = behavior_scores[sub_n-1, run_n-1]\n",
    "            eeg_xarray = xr.DataArray(\n",
    "                data=eeg_data,\n",
    "                dims=['time', 'channel'],\n",
    "                coords={\n",
    "                    'time': time_steps,\n",
    "                },\n",
    "                attrs={\n",
    "                    'schema_ref': 'eeg_signal',\n",
    "                    'data_name': f'subject_{sub_n}_run_{run_n}_channels',\n",
    "                    'subject': f'subject_{sub_n}',\n",
    "                    'session_data_ref': {'schema_ref': 'session', 'data_name': f'session_{run_n}'},\n",
    "                    'sampling_frequency': fs,\n",
    "                    'attending_direction': attending_direction,\n",
    "                    'attending_story': attending_story,\n",
    "                    'attend_score': float(behavior_score[0]),\n",
    "                    'nonattend_score': float(behavior_score[1])\n",
    "                }\n",
    "            )\n",
    "            mastoid_xarray = xr.DataArray(\n",
    "                data=mastoid_data,\n",
    "                dims=['time', 'channel'],\n",
    "                coords={\n",
    "                    'time': time_steps,\n",
    "                    'channel': ['left', 'right']\n",
    "                },\n",
    "                attrs={\n",
    "                    'schema_ref': 'eeg_signal',\n",
    "                    'data_name': f'subject_{sub_n}_run_{run_n}_mastoid',\n",
    "                    'subject': f'subject_{sub_n}',\n",
    "                    'session_data_ref': {'schema_ref': 'session', 'data_name': f'session_{run_n}'},\n",
    "                    'sampling_frequency': fs,\n",
    "                    'attending_direction': attending_direction,\n",
    "                    'attending_story': attending_story,\n",
    "                    'attend_score': float(behavior_score[0]),\n",
    "                    'nonattend_score': float(behavior_score[1])\n",
    "                }\n",
    "            )\n",
    "            yield eeg_xarray, mastoid_xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T01:06:38.986993100Z",
     "start_time": "2024-07-03T01:06:38.906202Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_stimuli(data_path):\n",
    "    stimuli_dir = data_path / \"Stimuli\"\n",
    "    envelopes_20000_dir = stimuli_dir / \"Envelopes\" / \"20000\"\n",
    "    envelopes_20000_files = os.listdir(envelopes_20000_dir)\n",
    "    envelopes_journey_dir = stimuli_dir / \"Envelopes\" / \"Journey\"\n",
    "    envelopes_journey_files = os.listdir(envelopes_journey_dir)\n",
    "    text_20000_dir = stimuli_dir / \"Text\" / \"20000\"\n",
    "    text_20000_files = os.listdir(text_20000_dir)\n",
    "    text_journey_dir = stimuli_dir / \"Text\" / \"Journey\"\n",
    "    text_journey_files = os.listdir(text_journey_dir)\n",
    "    assert len(envelopes_20000_files) == len(text_20000_files)\n",
    "    assert len(envelopes_journey_files) == len(text_journey_files)\n",
    "    assert len(envelopes_20000_files) == len(envelopes_journey_files)\n",
    "    n_runs = len(envelopes_20000_files)\n",
    "    for run in range(n_runs):\n",
    "        stimuli_record = {\n",
    "            'schema_ref': 'stimuli_record',\n",
    "            'data_name': f'session_{run}',\n",
    "            'left_wordvec_data_ref': {'schema_ref': 'wordvec', 'data_name': f'20000_run_{run}'},\n",
    "            'left_offset_time_data_ref': {'schema_ref': 'offset_times', 'data_name': f'20000_run_{run}'},\n",
    "            'left_onset_time_data_ref': {'schema_ref': 'onset_times', 'data_name': f'20000_run_{run}'},\n",
    "            'left_sentence_boundaries_data_ref': {'schema_ref': 'sentence_boundaries', 'data_name': f'20000_run_{run}'},\n",
    "            'left_envelope_data_ref': {'schema_ref': 'envelope', 'data_name': f'20000_run_{run}'},\n",
    "            'right_wordvec_data_ref': {'schema_ref': 'wordvec', 'data_name': f'journey_run_{run}'},\n",
    "            'right_offet_time_data_ref': {'schema_ref': 'offset_times', 'data_name': f'journey_run_{run}'},\n",
    "            'right_onset_time_data_ref': {'schema_ref': 'onset_times', 'data_name': f'journey_run_{run}'},\n",
    "            'right_sentence_boundaries_data_ref': {'schema_ref': 'sentence_boundaries', 'data_name': f'journey_run_{run}'},\n",
    "            'right_envelope_data_ref': {'schema_ref': 'envelope', 'data_name': f'journey_run_{run}'}\n",
    "        }\n",
    "        # text contains keys: 'offset_time', 'onset_time', 'sentence_boundaries', 'wordVec'\n",
    "        text_20000 = sio.loadmat(text_20000_dir / text_20000_files[run])\n",
    "        wordvec_20000 = xr.Dataset(\n",
    "            {\n",
    "                'wordVec': (['time'], text_20000['wordVec']),\n",
    "                'onset_time': (['time'], text_20000['onset_time'].flatten()),\n",
    "                'offset_time': (['time'], text_20000['offset_time'].flatten())\n",
    "            },\n",
    "\n",
    "        )\n",
    "        text_journey = sio.loadmat(text_journey_dir / text_journey_files[run])\n",
    "        # envelopes contains keys: 'envelope', 'fsEnv', 'origLength'\n",
    "        envelopes_20000 = sio.loadmat(envelopes_20000_dir / envelopes_20000_files[run])\n",
    "        envelopes_journey = sio.loadmat(envelopes_journey_dir / envelopes_journey_files[run])\n",
    "        left_wordvec = xr.DataArray(\n",
    "            wordvec_20000['wordVec'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'wordvec',\n",
    "                'data_name': f'2000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_wordvec = xr.DataArray(\n",
    "            text_journey['wordVec'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'wordvec',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'story': 'Journey to the Center of the Earth',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_onset_time = xr.DataArray(\n",
    "            text_20000['onset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'onset_times',\n",
    "                'data_name': f'2000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_onset_time = xr.DataArray(\n",
    "            text_journey['onset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'onset_times',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'story': 'Journey to the Center of the Earth',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_offset_time = xr.DataArray(\n",
    "            text_20000['offset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'offset_times',\n",
    "                'data_name': f'2000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_offset_time = xr.DataArray(\n",
    "            text_journey['offset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'offset_times',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'story': 'Journey to the Center of the Earth',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_sentence_boundaries = xr.DataArray(\n",
    "            text_20000['sentence_boundaries'],\n",
    "            dims = ['sentence_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'sentence_boundaries',\n",
    "                'data_name': f'2000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_sentence_boundaries = xr.DataArray(\n",
    "            text_journey['sentence_boundaries'],\n",
    "            dims = ['sentence_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'sentence_boundaries',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_envelope = xr.DataArray(\n",
    "            envelopes_20000['envelope'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'envelope',\n",
    "                'data_name': f'2000_run_{run}',\n",
    "                'sampling_frequency': envelopes_20000['fsEnv'],\n",
    "                'original_length': envelopes_20000['origLength'],\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_envelope = xr.DataArray(\n",
    "            envelopes_journey['envelope'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'envelope',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'sampling_frequency': envelopes_journey['fsEnv'],\n",
    "                'original_length': envelopes_journey['origLength'],\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        yield left_wordvec, right_wordvec, left_onset_time, right_onset_time, left_offset_time, right_offset_time, left_sentence_boundaries, right_sentence_boundaries, left_envelope, right_envelope, stimuli_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T01:06:40.646413100Z",
     "start_time": "2024-07-03T01:06:40.429879900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\tmpf4ec5ldg\n",
      "record_metamodel\n",
      "xarray_dataarray_metamodel\n",
      "version_timestamp\n",
      "schema_ref\n",
      "schema_type\n",
      "schema_name\n",
      "schema_title\n",
      "schema_description\n",
      "data_name\n",
      "time_of_save\n",
      "time_of_removal\n",
      "record_type\n",
      "json_schema\n",
      "has_file\n",
      "unit_of_measure\n",
      "dimension_of_measure\n",
      "acquisition\n",
      "acquisition_date\n",
      "import_date\n",
      "acquisition_notes\n",
      "data_dimensions\n",
      "shape\n",
      "dtype\n",
      "session_description\n",
      "session_date\n",
      "session_time\n",
      "session_duration\n",
      "session_notes\n",
      "data_ref\n",
      "start_time\n",
      "duration\n",
      "duration_unit\n",
      "animal_species\n",
      "age\n",
      "age_unit\n",
      "age_lower_bound\n",
      "age_upper_bound\n",
      "animal_id\n",
      "tetrode_id\n",
      "tetrode_depth\n",
      "genotype\n",
      "animal_strain\n",
      "stimulus_type\n",
      "stimulus_id\n",
      "stimulus_description\n",
      "recording_length\n",
      "sample_rate\n",
      "arena_shape\n",
      "arena_description\n",
      "study_description\n",
      "arena_height\n",
      "arena_width\n",
      "diameter\n",
      "arena_side_length\n",
      "arena_radius\n",
      "spike_count\n",
      "subject\n",
      "sampling_frequency\n",
      "attending_direction\n",
      "attending_story\n",
      "attend_score\n",
      "nonattend_score\n",
      "original_length\n",
      "eeg_signal\n",
      "session\n",
      "stimuli_record\n",
      "wordvec\n",
      "offset_times\n",
      "onset_times\n",
      "sentence_boundaries\n",
      "envelope\n"
     ]
    }
   ],
   "source": [
    "filesystem = LocalFileSystem()\n",
    "tmp_dir = TemporaryDirectory()\n",
    "print(tmp_dir.name)\n",
    "tmp_dir_fs = DirFileSystem(\n",
    "    tmp_dir.name,\n",
    "    filesystem=filesystem\n",
    ")\n",
    "client = MongoClient()\n",
    "memory_store = {}\n",
    "uow_provider = UnitOfWorkProvider(\n",
    "    mongo_client=client,\n",
    "    filesystem=filesystem,\n",
    "    memory_store=memory_store\n",
    ")\n",
    "import json\n",
    "cwd = Path.cwd()\n",
    "domain_models_path = cwd.parent / \"DomainModels\\\\cocktail_party\\\\data_models.json\"\n",
    "metamodel_path = cwd.parent / \"DomainModels\\\\cocktail_party\\\\metamodels.json\"\n",
    "property_path = cwd.parent / \"DomainModels\\\\cocktail_party\\\\property_models.json\"\n",
    "\n",
    "with open(metamodel_path) as f:\n",
    "    metamodels = json.load(f)\n",
    "\n",
    "with open(property_path) as f:\n",
    "    property_models = json.load(f)\n",
    "    \n",
    "# for metamodel in metamodels:\n",
    "# with uow_provider('cocktail-party') as uow:\n",
    "#     print(f\"Adding model {metamodel['schema_name']} to domain_models store.\")\n",
    "#     uow.domain_models.add(metamodel)\n",
    "#     model = uow.domain_models.get(metamodel['schema_name'])\n",
    "#     print(model['schema_name'])\n",
    "#     uow.commit()\n",
    "\n",
    "# load domain models json file\n",
    "with open(domain_models_path) as f:\n",
    "    domain_models = json.load(f)\n",
    "    \n",
    "with uow_provider('cocktail-party') as uow:\n",
    "    for metamodel in metamodels:\n",
    "        uow.domain_models.add(metamodel)\n",
    "        model = uow.domain_models.get(metamodel['schema_name'])\n",
    "        print(model['schema_name'])\n",
    "    for property_model in property_models:\n",
    "        uow.domain_models.add(property_model)\n",
    "        model = uow.domain_models.get(property_model['schema_name'])\n",
    "        print(model['schema_name'])\n",
    "    for domain_model in domain_models:\n",
    "        uow.domain_models.add(domain_model)\n",
    "        model = uow.domain_models.get(domain_model['schema_name'])\n",
    "        print(model['schema_name'])\n",
    "        uow.commit()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileSystemDAOFileAlreadyExistsError",
     "evalue": "Cannot add object with path \"cocktail-party/eeg_signal__subject_14_run_23_mastoid.nc\" because it already exists in repository.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileSystemDAOFileAlreadyExistsError\u001B[0m       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m uow_provider(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcocktail-party\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m uow:\n\u001B[0;32m      3\u001B[0m     uow\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39madd(eeg_xarray)\n\u001B[1;32m----> 4\u001B[0m     \u001B[43muow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmastoid_xarray\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     uow\u001B[38;5;241m.\u001B[39mcommit()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SQL_py_read\\lib\\site-packages\\signalstore\\store\\repositories.py:591\u001B[0m, in \u001B[0;36mDataRepository.add\u001B[1;34m(self, object, data_adapter, versioning_on)\u001B[0m\n\u001B[0;32m    589\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39mattrs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m), dttype):\n\u001B[0;32m    590\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m DataRepositoryTypeError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mversion_timestamp\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m must be a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdttype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m object or the integer 0, not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39mattrs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mversion_timestamp\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 591\u001B[0m     ohe \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_data_with_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mobject\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    593\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_timestamp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_timestamp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    594\u001B[0m \u001B[43m        \u001B[49m\u001B[43mversioning_on\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mversioning_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    595\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_adapter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_adapter\u001B[49m\n\u001B[0;32m    596\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    597\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ohe\n\u001B[0;32m    598\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SQL_py_read\\lib\\site-packages\\signalstore\\store\\repositories.py:638\u001B[0m, in \u001B[0;36mDataRepository._add_data_with_file\u001B[1;34m(self, object, add_timestamp, versioning_on, data_adapter)\u001B[0m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate(\u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39mattrs)\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_records\u001B[38;5;241m.\u001B[39madd(\n\u001B[0;32m    634\u001B[0m     document\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39mattrs,\n\u001B[0;32m    635\u001B[0m     timestamp\u001B[38;5;241m=\u001B[39mohe\u001B[38;5;241m.\u001B[39mtimestamp,\n\u001B[0;32m    636\u001B[0m     versioning_on\u001B[38;5;241m=\u001B[39mversioning_on\n\u001B[0;32m    637\u001B[0m     )\n\u001B[1;32m--> 638\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    639\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_object\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mobject\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    640\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_adapter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_adapter\u001B[49m\n\u001B[0;32m    641\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    642\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_operation_history\u001B[38;5;241m.\u001B[39mappend(ohe)\n\u001B[0;32m    643\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ohe\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SQL_py_read\\lib\\site-packages\\signalstore\\store\\data_access_objects.py:565\u001B[0m, in \u001B[0;36mFileSystemDAO.add\u001B[1;34m(self, data_object, data_adapter)\u001B[0m\n\u001B[0;32m    563\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_filepath(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39midkwargs, data_adapter\u001B[38;5;241m=\u001B[39mdata_adapter)\n\u001B[0;32m    564\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexists(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39midkwargs, data_adapter\u001B[38;5;241m=\u001B[39mdata_adapter):\n\u001B[1;32m--> 565\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m FileSystemDAOFileAlreadyExistsError(\n\u001B[0;32m    566\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCannot add object with path \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m because it already exists in repository.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    567\u001B[0m     )\n\u001B[0;32m    568\u001B[0m data_object \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialize(data_object)\n\u001B[0;32m    569\u001B[0m \u001B[38;5;66;03m#get os environment variable 'DEBUG' to check if we should print the data_object\u001B[39;00m\n",
      "\u001B[1;31mFileSystemDAOFileAlreadyExistsError\u001B[0m: Cannot add object with path \"cocktail-party/eeg_signal__subject_14_run_23_mastoid.nc\" because it already exists in repository."
     ]
    }
   ],
   "source": [
    "for eeg_xarray, mastoid_xarray in load_eeg_data(data_path):\n",
    "    with uow_provider('cocktail-party') as uow:\n",
    "        uow.data.add(eeg_xarray)\n",
    "        uow.data.add(mastoid_xarray)\n",
    "        \n",
    "        uow.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T01:10:08.241309400Z",
     "start_time": "2024-07-03T01:07:46.767698100Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for result in load_stimuli(data_path):\n",
    "    for data in result:\n",
    "        with uow_provider('cocktail-party') as uow:\n",
    "            uow.data.add(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envelopes_path = \"D:\\BlcRepo\\OtherCode\\Generative_Neuroscience\\Dataset\\Broderick\\Cocktail Party\\Stimuli\\Envelopes\\\\20000\\\\20000_1_env.mat\"\n",
    "envelopes_path = os.path.join(data_path, \"Stimuli\", \"Envelopes\", \"20000\", \"20000_1_env.mat\")\n",
    "matf = sio.loadmat(envelopes_path)\n",
    "print(matf.keys())\n",
    "print(matf['envelope'].shape)\n",
    "print(matf['fsEnv'])\n",
    "print(matf['origLength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_path = \"K:\\ke\\sta\\data\\SpeechEEG\\Cocktail Party\\Stimuli\\Text\\\\20000\\Run4.mat\"\n",
    "text_path = os.path.join(data_path, \"Stimuli\", \"Text\", \"20000\", \"Run4.mat\")\n",
    "matf = sio.loadmat(text_path)\n",
    "print(matf.keys())\n",
    "def utf_array_to_str(arr):\n",
    "    string = \"\"\n",
    "    for word in arr:\n",
    "        string += word[0][0] + \" \"\n",
    "    return string\n",
    "print(utf_array_to_str(matf['wordVec']))\n",
    "print(matf['wordVec'].shape)\n",
    "print(matf['onset_time'].shape)\n",
    "print(matf['offset_time'].shape)\n",
    "print(matf['sentence_boundaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def compute_unique_vocabulary(text_dir):\n",
    "    full_paths = [text_dir / \"20000\" / f for f in os.listdir(text_dir / \"20000\")] \\\n",
    "                + [text_dir / \"Journey\" / f for f in os.listdir(text_dir / \"Journey\")]\n",
    "    word_counts = {}\n",
    "    word_lookup = {}\n",
    "    for text_file in full_paths:\n",
    "        matf = sio.loadmat(text_file)\n",
    "        run = text_file.stem.replace(\"Run\", \"\")\n",
    "        source = text_file.parent.stem\n",
    "        for word in matf['wordVec']:\n",
    "            for word in word:\n",
    "                word = str(word[0])\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                    word_lookup[word] += [(source, f'session_{run}')]\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "                    word_lookup[word] = [(source, f'session_{run}')]\n",
    "    return word_counts, word_lookup\n",
    "vocab, lookup = compute_unique_vocabulary(data_path / \"Stimuli\" / \"Text\")\n",
    "print(len(vocab))\n",
    "print(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
