{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:13:07.844415800Z",
     "start_time": "2024-07-03T02:13:07.778989Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To be able to make edits to repo without having to restart notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:13:13.464613800Z",
     "start_time": "2024-07-03T02:13:11.773782600Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_path = Path(\"D:\\BlcRepo\\OtherCode\\Generative_Neuroscience\\Dataset\\Broderick\\Cocktail Party\")\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from signalstore import UnitOfWorkProvider\n",
    "from mongomock import MongoClient\n",
    "#from pymongo import MongoClient\n",
    "from fsspec.implementations.local import LocalFileSystem\n",
    "from fsspec import get_mapper\n",
    "from fsspec.implementations.dirfs import DirFileSystem\n",
    "from tempfile import TemporaryDirectory\n",
    "import fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cocktail Party Experiment\n",
    "\n",
    "## EEG Files\n",
    "eegData: EEG Data, Time Locked to the onset of the speech stimulus.   \n",
    "Format: Channels (128) x Time Points\n",
    "\n",
    "mastoids: Mastoid Channels, Time Locked to the onset of the speech stimulus. \n",
    "Format: Channels (Left=1 Right=2) x Time Points\n",
    "\n",
    "fs: Sampling Rate \n",
    "\n",
    "EEG data is unfiltered, unreferenced and sampled at 128Hz\n",
    "\n",
    "## Experiment Information\n",
    "Subjects 1-17 were instructed to attend to 'Twenty Thousand Leagues Under the Sea' (20000), played in the left ear\n",
    "Subjects 18-33 were instructed to attend to 'Journey to the Centre of the Earth' (Journey), played in the right ear\n",
    "\n",
    "## Behavioral Data\n",
    "score: Comprehension question scores for attended and unattended stories.\n",
    "Format: Subjects x Run x Story (1=Attended, 2=Unattended)\n",
    "\n",
    "## Stimuli Data Files\n",
    "\n",
    "wordVec = List of all the content words for a given trial\n",
    "onset_time = Onset time of the word in the corresponding cell of 'wordVec' (given in seconds)\n",
    "offset_time = Offset time of the word in the corresponding cell of 'wordVec' (given in seconds)\n",
    "sentence_boundaries = Time of sentence close (in seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:19:35.666921500Z",
     "start_time": "2024-07-03T02:19:35.573455400Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_eeg_data(data_path):\n",
    "    eeg_dir = data_path / \"EEG\"\n",
    "    subjects = [sdir for sdir in os.listdir(eeg_dir) if os.path.isdir(eeg_dir / sdir)]\n",
    "    behavior_path = data_path / \"Behavioural Data\" / \"Comprehension Scores.mat\"\n",
    "    behavior_data = sio.loadmat(behavior_path)\n",
    "    behavior_scores = behavior_data['score']\n",
    "    for sub_n, subject in enumerate(subjects):\n",
    "        sub_n += 1\n",
    "        subject_dir = eeg_dir / subject\n",
    "        # list run mat files\n",
    "        runs = os.listdir(subject_dir)\n",
    "        for run_n, run in enumerate(runs):\n",
    "            run_n += 1\n",
    "            mfile = sio.loadmat(subject_dir / run)\n",
    "            eeg_data = np.array(mfile['eegData'])\n",
    "            # print(eeg_data.shape)\n",
    "            mastoid_data = mfile['mastoids']\n",
    "            try:\n",
    "                assert mastoid_data.shape[1] == 2\n",
    "            except AssertionError:\n",
    "                print(f\"Subject {sub_n} Run {run_n} has mastoid data shape {mastoid_data.shape}\")\n",
    "                print(f\"Subject {sub_n} Run {run_n} has eeg data shape {eeg_data.shape}\")\n",
    "                mastoid_data = mastoid_data.T\n",
    "                eeg_data = eeg_data.T\n",
    "            \n",
    "            fs = int(mfile['fs'][0][0])\n",
    "            max_time = eeg_data.shape[0] / fs\n",
    "            time_steps = np.linspace(0, max_time, eeg_data.shape[0]).squeeze() # in seconds\n",
    "            if sub_n <= 17:\n",
    "                attending_direction = 'left'\n",
    "                attending_story = 'Twenty Thousand Leagues Under the Sea'\n",
    "            else:\n",
    "                attending_direction = 'right'\n",
    "                attending_story = 'Journey to the Center of the Earth'\n",
    "            behavior_score = behavior_scores[sub_n-1, run_n-1]\n",
    "            eeg_xarray = xr.DataArray(\n",
    "                data=eeg_data,\n",
    "                dims=['time', 'channel'],\n",
    "                coords={\n",
    "                    'time': time_steps,\n",
    "                },\n",
    "                attrs={\n",
    "                    'schema_ref': 'eeg_signal',\n",
    "                    'data_name': f'subject_{sub_n}_run_{run_n}_channels',\n",
    "                    'subject': f'subject_{sub_n}',\n",
    "                    'session_data_ref': {'schema_ref': 'session', 'data_name': f'session_{run_n}'},\n",
    "                    'sampling_frequency': fs,\n",
    "                    'attending_direction': attending_direction,\n",
    "                    'attending_story': attending_story,\n",
    "                    'attend_score': float(behavior_score[0]),\n",
    "                    'nonattend_score': float(behavior_score[1])\n",
    "                }\n",
    "            )\n",
    "            mastoid_xarray = xr.DataArray(\n",
    "                data=mastoid_data,\n",
    "                dims=['time', 'channel'],\n",
    "                coords={\n",
    "                    'time': time_steps,\n",
    "                    'channel': ['left', 'right']\n",
    "                },\n",
    "                attrs={\n",
    "                    'schema_ref': 'eeg_signal',\n",
    "                    'data_name': f'subject_{sub_n}_run_{run_n}_mastoid',\n",
    "                    'subject': f'subject_{sub_n}',\n",
    "                    'session_data_ref': {'schema_ref': 'session', 'data_name': f'session_{run_n}'},\n",
    "                    'sampling_frequency': fs,\n",
    "                    'attending_direction': attending_direction,\n",
    "                    'attending_story': attending_story,\n",
    "                    'attend_score': float(behavior_score[0]),\n",
    "                    'nonattend_score': float(behavior_score[1])\n",
    "                }\n",
    "            )\n",
    "            yield eeg_xarray, mastoid_xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:13:14.825140600Z",
     "start_time": "2024-07-03T02:13:14.757322400Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_stimuli(data_path):\n",
    "    stimuli_dir = data_path / \"Stimuli\"\n",
    "    envelopes_20000_dir = stimuli_dir / \"Envelopes\" / \"20000\"\n",
    "    envelopes_20000_files = os.listdir(envelopes_20000_dir)\n",
    "    envelopes_journey_dir = stimuli_dir / \"Envelopes\" / \"Journey\"\n",
    "    envelopes_journey_files = os.listdir(envelopes_journey_dir)\n",
    "    text_20000_dir = stimuli_dir / \"Text\" / \"20000\"\n",
    "    text_20000_files = os.listdir(text_20000_dir)\n",
    "    text_journey_dir = stimuli_dir / \"Text\" / \"Journey\"\n",
    "    text_journey_files = os.listdir(text_journey_dir)\n",
    "    assert len(envelopes_20000_files) == len(text_20000_files)\n",
    "    assert len(envelopes_journey_files) == len(text_journey_files)\n",
    "    assert len(envelopes_20000_files) == len(envelopes_journey_files)\n",
    "    n_runs = len(envelopes_20000_files)\n",
    "    for run in range(n_runs):\n",
    "        stimuli_record = {\n",
    "            'schema_ref': 'stimuli_record',\n",
    "            'data_name': f'session_{run}',\n",
    "            'left_wordvec_data_ref': {'schema_ref': 'wordvec', 'data_name': f'20000_run_{run}'},\n",
    "            'left_offset_time_data_ref': {'schema_ref': 'offset_times', 'data_name': f'20000_run_{run}'},\n",
    "            'left_onset_time_data_ref': {'schema_ref': 'onset_times', 'data_name': f'20000_run_{run}'},\n",
    "            'left_sentence_boundaries_data_ref': {'schema_ref': 'sentence_boundaries', 'data_name': f'20000_run_{run}'},\n",
    "            'left_envelope_data_ref': {'schema_ref': 'envelope', 'data_name': f'20000_run_{run}'},\n",
    "            'right_wordvec_data_ref': {'schema_ref': 'wordvec', 'data_name': f'journey_run_{run}'},\n",
    "            'right_offet_time_data_ref': {'schema_ref': 'offset_times', 'data_name': f'journey_run_{run}'},\n",
    "            'right_onset_time_data_ref': {'schema_ref': 'onset_times', 'data_name': f'journey_run_{run}'},\n",
    "            'right_sentence_boundaries_data_ref': {'schema_ref': 'sentence_boundaries', 'data_name': f'journey_run_{run}'},\n",
    "            'right_envelope_data_ref': {'schema_ref': 'envelope', 'data_name': f'journey_run_{run}'}\n",
    "        }\n",
    "        # text contains keys: 'offset_time', 'onset_time', 'sentence_boundaries', 'wordVec'\n",
    "        text_20000 = sio.loadmat(text_20000_dir / text_20000_files[run])\n",
    "        wordvec_20000_data = np.array(text_20000['wordVec'].tolist()).flatten()\n",
    "        wordvec_20000 = xr.Dataset(\n",
    "            {\n",
    "                'wordVec': (['time'], wordvec_20000_data),\n",
    "                'onset_time': (['time'], text_20000['onset_time'].flatten()),\n",
    "                'offset_time': (['time'], text_20000['offset_time'].flatten())\n",
    "            },\n",
    "\n",
    "        )\n",
    "        text_journey = sio.loadmat(text_journey_dir / text_journey_files[run])\n",
    "        wordvec_journey_data = np.array(text_journey['wordVec'].tolist()).flatten()\n",
    "        wordvec_journey = xr.Dataset(\n",
    "            {\n",
    "                'wordVec': (['time'], wordvec_journey_data),\n",
    "                'onset_time': (['time'], text_journey['onset_time'].flatten()),\n",
    "                'offset_time': (['time'], text_journey['offset_time'].flatten())\n",
    "            },\n",
    "\n",
    "        )\n",
    "        \n",
    "        # envelopes contains keys: 'envelope', 'fsEnv', 'origLength'\n",
    "        envelopes_20000 = sio.loadmat(envelopes_20000_dir / envelopes_20000_files[run])\n",
    "        envelopes_journey = sio.loadmat(envelopes_journey_dir / envelopes_journey_files[run])\n",
    "        \n",
    "        left_wordvec = xr.DataArray(\n",
    "            wordvec_20000['wordVec'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'wordvec',\n",
    "                'data_name': f'20000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_wordvec = xr.DataArray(\n",
    "            wordvec_journey['wordVec'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'wordvec',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'story': 'Journey to the Center of the Earth',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_onset_time = xr.DataArray(\n",
    "            wordvec_20000['onset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'onset_times',\n",
    "                'data_name': f'20000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_onset_time = xr.DataArray(\n",
    "            wordvec_journey['onset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'onset_times',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'story': 'Journey to the Center of the Earth',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_offset_time = xr.DataArray(\n",
    "            wordvec_20000['offset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'offset_times',\n",
    "                'data_name': f'20000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_offset_time = xr.DataArray(\n",
    "            wordvec_journey['offset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'offset_times',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'story': 'Journey to the Center of the Earth',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_sentence_boundaries = xr.DataArray(\n",
    "            text_20000['sentence_boundaries'].flatten(),\n",
    "            dims = ['sentence_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'sentence_boundaries',\n",
    "                'data_name': f'20000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_sentence_boundaries = xr.DataArray(\n",
    "            text_journey['sentence_boundaries'].flatten(),\n",
    "            dims = ['sentence_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'sentence_boundaries',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_envelope = xr.DataArray(\n",
    "            envelopes_20000['envelope'].flatten(),\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'envelope',\n",
    "                'data_name': f'20000_run_{run}',\n",
    "                'sampling_frequency': int(envelopes_20000['fsEnv'][0][0]),\n",
    "                'original_length': int(envelopes_20000['origLength'][0][0]),\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_envelope = xr.DataArray(\n",
    "            envelopes_journey['envelope'].flatten(),\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'envelope',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'sampling_frequency': int(envelopes_journey['fsEnv'][0][0]),\n",
    "                'original_length': int(envelopes_journey['origLength'][0][0]),\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        yield left_wordvec, right_wordvec, left_onset_time, right_onset_time, left_offset_time, right_offset_time, left_sentence_boundaries, right_sentence_boundaries, left_envelope, right_envelope, stimuli_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:13:16.127758Z",
     "start_time": "2024-07-03T02:13:15.907346500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_metamodel\n",
      "xarray_dataarray_metamodel\n",
      "version_timestamp\n",
      "schema_ref\n",
      "schema_type\n",
      "schema_name\n",
      "schema_title\n",
      "schema_description\n",
      "data_name\n",
      "time_of_save\n",
      "time_of_removal\n",
      "record_type\n",
      "json_schema\n",
      "has_file\n",
      "unit_of_measure\n",
      "dimension_of_measure\n",
      "acquisition\n",
      "acquisition_date\n",
      "import_date\n",
      "acquisition_notes\n",
      "data_dimensions\n",
      "shape\n",
      "dtype\n",
      "session_description\n",
      "session_date\n",
      "session_time\n",
      "session_duration\n",
      "session_notes\n",
      "data_ref\n",
      "start_time\n",
      "duration\n",
      "duration_unit\n",
      "animal_species\n",
      "age\n",
      "age_unit\n",
      "age_lower_bound\n",
      "age_upper_bound\n",
      "animal_id\n",
      "tetrode_id\n",
      "tetrode_depth\n",
      "genotype\n",
      "animal_strain\n",
      "stimulus_type\n",
      "stimulus_id\n",
      "stimulus_description\n",
      "recording_length\n",
      "sample_rate\n",
      "arena_shape\n",
      "arena_description\n",
      "study_description\n",
      "arena_height\n",
      "arena_width\n",
      "diameter\n",
      "arena_side_length\n",
      "arena_radius\n",
      "spike_count\n",
      "subject\n",
      "sampling_frequency\n",
      "attending_direction\n",
      "attending_story\n",
      "attend_score\n",
      "nonattend_score\n",
      "original_length\n",
      "story\n",
      "eeg_signal\n",
      "session\n",
      "stimuli_record\n",
      "wordvec\n",
      "offset_times\n",
      "onset_times\n",
      "sentence_boundaries\n",
      "envelope\n"
     ]
    }
   ],
   "source": [
    "filesystem = LocalFileSystem()\n",
    "# tmp_dir = TemporaryDirectory()\n",
    "# print(tmp_dir.name)\n",
    "\n",
    "# Create data storage location\n",
    "dataset_name = \"cocktail_party\"\n",
    "store_path = Path(\"D:\\BlcRepo\\OtherCode\\Generative_Neuroscience\\Temp_Dataset\")\n",
    "\n",
    "# Create a directory for the dataset\n",
    "if not os.path.exists(store_path):\n",
    "    os.makedirs(store_path)\n",
    "\n",
    "tmp_dir_fs = DirFileSystem(\n",
    "    store_path,\n",
    "    filesystem=filesystem\n",
    ")\n",
    "client = MongoClient()\n",
    "memory_store = {}\n",
    "uow_provider = UnitOfWorkProvider(\n",
    "    mongo_client=client,\n",
    "    filesystem=tmp_dir_fs,\n",
    "    memory_store=memory_store\n",
    ")\n",
    "import json\n",
    "cwd = Path.cwd()\n",
    "domain_models_path = cwd.parent / f\"DomainModels\\\\{dataset_name}\\\\data_models.json\"\n",
    "metamodel_path = cwd.parent / f\"DomainModels\\\\{dataset_name}\\\\metamodels.json\"\n",
    "property_path = cwd.parent / f\"DomainModels\\\\{dataset_name}\\\\property_models.json\"\n",
    "\n",
    "with open(metamodel_path) as f:\n",
    "    metamodels = json.load(f)\n",
    "\n",
    "with open(property_path) as f:\n",
    "    property_models = json.load(f)\n",
    "    \n",
    "# for metamodel in metamodels:\n",
    "# with uow_provider('cocktail-party') as uow:\n",
    "#     print(f\"Adding model {metamodel['schema_name']} to domain_models store.\")\n",
    "#     uow.domain_models.add(metamodel)\n",
    "#     model = uow.domain_models.get(metamodel['schema_name'])\n",
    "#     print(model['schema_name'])\n",
    "#     uow.commit()\n",
    "\n",
    "# load domain models json file\n",
    "with open(domain_models_path) as f:\n",
    "    domain_models = json.load(f)\n",
    "    \n",
    "with uow_provider(dataset_name) as uow:\n",
    "    for metamodel in metamodels:\n",
    "        uow.domain_models.add(metamodel)\n",
    "        model = uow.domain_models.get(metamodel['schema_name'])\n",
    "        print(model['schema_name'])\n",
    "    for property_model in property_models:\n",
    "        uow.domain_models.add(property_model)\n",
    "        model = uow.domain_models.get(property_model['schema_name'])\n",
    "        print(model['schema_name'])\n",
    "    for domain_model in domain_models:\n",
    "        uow.domain_models.add(domain_model)\n",
    "        model = uow.domain_models.get(domain_model['schema_name'])\n",
    "        print(model['schema_name'])\n",
    "        uow.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Subject 30 Run 2 has data shape (2, 7681), and eeg data shape (128, 7681) for some reason, added the code in the load_eeg_data function to transpose the mastoid data if the second dimension is not 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:23:15.444866400Z",
     "start_time": "2024-07-03T02:19:59.783246300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 30 Run 2 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 2 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 3 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 3 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 4 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 4 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 5 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 5 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 6 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 6 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 7 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 7 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 8 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 8 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 9 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 9 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 10 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 10 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 11 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 11 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 13 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 13 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 14 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 14 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 15 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 15 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 16 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 16 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 17 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 17 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 18 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 18 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 19 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 19 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 20 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 20 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 21 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 21 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 22 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 22 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 24 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 24 has eeg data shape (128, 7681)\n",
      "Subject 30 Run 29 has mastoid data shape (2, 7681)\n",
      "Subject 30 Run 29 has eeg data shape (128, 7681)\n"
     ]
    }
   ],
   "source": [
    "for eeg_xarray, mastoid_xarray in load_eeg_data(data_path):\n",
    "    with uow_provider(dataset_name) as uow:\n",
    "        uow.data.add(eeg_xarray)\n",
    "        uow.data.add(mastoid_xarray)\n",
    "        \n",
    "        uow.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T02:13:28.660948Z",
     "start_time": "2024-07-03T02:13:22.305928200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for result in load_stimuli(data_path):\n",
    "    for data in result:\n",
    "        with uow_provider(dataset_name) as uow:\n",
    "            uow.data.add(data)\n",
    "            uow.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envelopes_path = \"D:\\BlcRepo\\OtherCode\\Generative_Neuroscience\\Dataset\\Broderick\\Cocktail Party\\Stimuli\\Envelopes\\\\20000\\\\20000_1_env.mat\"\n",
    "envelopes_path = os.path.join(data_path, \"Stimuli\", \"Envelopes\", \"20000\", \"20000_1_env.mat\")\n",
    "matf = sio.loadmat(envelopes_path)\n",
    "print(matf.keys())\n",
    "print(matf['envelope'].shape)\n",
    "print(matf['fsEnv'])\n",
    "print(matf['origLength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_path = \"K:\\ke\\sta\\data\\SpeechEEG\\Cocktail Party\\Stimuli\\Text\\\\20000\\Run4.mat\"\n",
    "text_path = os.path.join(data_path, \"Stimuli\", \"Text\", \"20000\", \"Run4.mat\")\n",
    "matf = sio.loadmat(text_path)\n",
    "print(matf.keys())\n",
    "def utf_array_to_str(arr):\n",
    "    string = \"\"\n",
    "    for word in arr:\n",
    "        string += word[0][0] + \" \"\n",
    "    return string\n",
    "print(utf_array_to_str(matf['wordVec']))\n",
    "print(matf['wordVec'].shape)\n",
    "print(matf['onset_time'].shape)\n",
    "print(matf['offset_time'].shape)\n",
    "print(matf['sentence_boundaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def compute_unique_vocabulary(text_dir):\n",
    "    full_paths = [text_dir / \"20000\" / f for f in os.listdir(text_dir / \"20000\")] \\\n",
    "                + [text_dir / \"Journey\" / f for f in os.listdir(text_dir / \"Journey\")]\n",
    "    word_counts = {}\n",
    "    word_lookup = {}\n",
    "    for text_file in full_paths:\n",
    "        matf = sio.loadmat(text_file)\n",
    "        run = text_file.stem.replace(\"Run\", \"\")\n",
    "        source = text_file.parent.stem\n",
    "        for word in matf['wordVec']:\n",
    "            for word in word:\n",
    "                word = str(word[0])\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                    word_lookup[word] += [(source, f'session_{run}')]\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "                    word_lookup[word] = [(source, f'session_{run}')]\n",
    "    return word_counts, word_lookup\n",
    "vocab, lookup = compute_unique_vocabulary(data_path / \"Stimuli\" / \"Text\")\n",
    "print(len(vocab))\n",
    "print(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
