{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# To be able to make edits to repo without having to restart notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T01:55:45.661929900Z",
     "start_time": "2024-07-03T01:55:45.622955600Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T01:55:48.680043700Z",
     "start_time": "2024-07-03T01:55:47.047197300Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_path = Path(\"D:\\BlcRepo\\OtherCode\\Generative_Neuroscience\\Dataset\\Broderick\\Cocktail Party\")\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from signalstore import UnitOfWorkProvider\n",
    "from mongomock import MongoClient\n",
    "#from pymongo import MongoClient\n",
    "from fsspec.implementations.local import LocalFileSystem\n",
    "from fsspec import get_mapper\n",
    "from fsspec.implementations.dirfs import DirFileSystem\n",
    "from tempfile import TemporaryDirectory\n",
    "import fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cocktail Party Experiment\n",
    "\n",
    "## EEG Files\n",
    "eegData: EEG Data, Time Locked to the onset of the speech stimulus.   \n",
    "Format: Channels (128) x Time Points\n",
    "\n",
    "mastoids: Mastoid Channels, Time Locked to the onset of the speech stimulus. \n",
    "Format: Channels (Left=1 Right=2) x Time Points\n",
    "\n",
    "fs: Sampling Rate \n",
    "\n",
    "EEG data is unfiltered, unreferenced and sampled at 128Hz\n",
    "\n",
    "## Experiment Information\n",
    "Subjects 1-17 were instructed to attend to 'Twenty Thousand Leagues Under the Sea' (20000), played in the left ear\n",
    "Subjects 18-33 were instructed to attend to 'Journey to the Centre of the Earth' (Journey), played in the right ear\n",
    "\n",
    "## Behavioral Data\n",
    "score: Comprehension question scores for attended and unattended stories.\n",
    "Format: Subjects x Run x Story (1=Attended, 2=Unattended)\n",
    "\n",
    "## Stimuli Data Files\n",
    "\n",
    "wordVec = List of all the content words for a given trial\n",
    "onset_time = Onset time of the word in the corresponding cell of 'wordVec' (given in seconds)\n",
    "offset_time = Offset time of the word in the corresponding cell of 'wordVec' (given in seconds)\n",
    "sentence_boundaries = Time of sentence close (in seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T01:55:53.849240800Z",
     "start_time": "2024-07-03T01:55:53.761446600Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_eeg_data(data_path):\n",
    "    eeg_dir = data_path / \"EEG\"\n",
    "    subjects = [sdir for sdir in os.listdir(eeg_dir) if os.path.isdir(eeg_dir / sdir)]\n",
    "    behavior_path = data_path / \"Behavioural Data\" / \"Comprehension Scores.mat\"\n",
    "    behavior_data = sio.loadmat(behavior_path)\n",
    "    behavior_scores = behavior_data['score']\n",
    "    for sub_n, subject in enumerate(subjects):\n",
    "        sub_n += 1\n",
    "        subject_dir = eeg_dir / subject\n",
    "        # list run mat files\n",
    "        runs = os.listdir(subject_dir)\n",
    "        for run_n, run in enumerate(runs):\n",
    "            run_n += 1\n",
    "            mfile = sio.loadmat(subject_dir / run)\n",
    "            eeg_data = np.array(mfile['eegData'])\n",
    "            # print(eeg_data.shape)\n",
    "            mastoid_data = mfile['mastoids']\n",
    "            fs = int(mfile['fs'][0][0])\n",
    "            max_time = eeg_data.shape[0] / fs\n",
    "            time_steps = np.linspace(0, max_time, eeg_data.shape[0]).squeeze()\n",
    "            if sub_n <= 17:\n",
    "                attending_direction = 'left'\n",
    "                attending_story = 'Twenty Thousand Leagues Under the Sea'\n",
    "            else:\n",
    "                attending_direction = 'right'\n",
    "                attending_story = 'Journey to the Center of the Earth'\n",
    "            behavior_score = behavior_scores[sub_n-1, run_n-1]\n",
    "            eeg_xarray = xr.DataArray(\n",
    "                data=eeg_data,\n",
    "                dims=['time', 'channel'],\n",
    "                coords={\n",
    "                    'time': time_steps,\n",
    "                },\n",
    "                attrs={\n",
    "                    'schema_ref': 'eeg_signal',\n",
    "                    'data_name': f'subject_{sub_n}_run_{run_n}_channels',\n",
    "                    'subject': f'subject_{sub_n}',\n",
    "                    'session_data_ref': {'schema_ref': 'session', 'data_name': f'session_{run_n}'},\n",
    "                    'sampling_frequency': fs,\n",
    "                    'attending_direction': attending_direction,\n",
    "                    'attending_story': attending_story,\n",
    "                    'attend_score': float(behavior_score[0]),\n",
    "                    'nonattend_score': float(behavior_score[1])\n",
    "                }\n",
    "            )\n",
    "            mastoid_xarray = xr.DataArray(\n",
    "                data=mastoid_data,\n",
    "                dims=['time', 'channel'],\n",
    "                coords={\n",
    "                    'time': time_steps,\n",
    "                    'channel': ['left', 'right']\n",
    "                },\n",
    "                attrs={\n",
    "                    'schema_ref': 'eeg_signal',\n",
    "                    'data_name': f'subject_{sub_n}_run_{run_n}_mastoid',\n",
    "                    'subject': f'subject_{sub_n}',\n",
    "                    'session_data_ref': {'schema_ref': 'session', 'data_name': f'session_{run_n}'},\n",
    "                    'sampling_frequency': fs,\n",
    "                    'attending_direction': attending_direction,\n",
    "                    'attending_story': attending_story,\n",
    "                    'attend_score': float(behavior_score[0]),\n",
    "                    'nonattend_score': float(behavior_score[1])\n",
    "                }\n",
    "            )\n",
    "            yield eeg_xarray, mastoid_xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T01:55:55.638949100Z",
     "start_time": "2024-07-03T01:55:55.550265900Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_stimuli(data_path):\n",
    "    stimuli_dir = data_path / \"Stimuli\"\n",
    "    envelopes_20000_dir = stimuli_dir / \"Envelopes\" / \"20000\"\n",
    "    envelopes_20000_files = os.listdir(envelopes_20000_dir)\n",
    "    envelopes_journey_dir = stimuli_dir / \"Envelopes\" / \"Journey\"\n",
    "    envelopes_journey_files = os.listdir(envelopes_journey_dir)\n",
    "    text_20000_dir = stimuli_dir / \"Text\" / \"20000\"\n",
    "    text_20000_files = os.listdir(text_20000_dir)\n",
    "    text_journey_dir = stimuli_dir / \"Text\" / \"Journey\"\n",
    "    text_journey_files = os.listdir(text_journey_dir)\n",
    "    assert len(envelopes_20000_files) == len(text_20000_files)\n",
    "    assert len(envelopes_journey_files) == len(text_journey_files)\n",
    "    assert len(envelopes_20000_files) == len(envelopes_journey_files)\n",
    "    n_runs = len(envelopes_20000_files)\n",
    "    for run in range(n_runs):\n",
    "        stimuli_record = {\n",
    "            'schema_ref': 'stimuli_record',\n",
    "            'data_name': f'session_{run}',\n",
    "            'left_wordvec_data_ref': {'schema_ref': 'wordvec', 'data_name': f'20000_run_{run}'},\n",
    "            'left_offset_time_data_ref': {'schema_ref': 'offset_times', 'data_name': f'20000_run_{run}'},\n",
    "            'left_onset_time_data_ref': {'schema_ref': 'onset_times', 'data_name': f'20000_run_{run}'},\n",
    "            'left_sentence_boundaries_data_ref': {'schema_ref': 'sentence_boundaries', 'data_name': f'20000_run_{run}'},\n",
    "            'left_envelope_data_ref': {'schema_ref': 'envelope', 'data_name': f'20000_run_{run}'},\n",
    "            'right_wordvec_data_ref': {'schema_ref': 'wordvec', 'data_name': f'journey_run_{run}'},\n",
    "            'right_offet_time_data_ref': {'schema_ref': 'offset_times', 'data_name': f'journey_run_{run}'},\n",
    "            'right_onset_time_data_ref': {'schema_ref': 'onset_times', 'data_name': f'journey_run_{run}'},\n",
    "            'right_sentence_boundaries_data_ref': {'schema_ref': 'sentence_boundaries', 'data_name': f'journey_run_{run}'},\n",
    "            'right_envelope_data_ref': {'schema_ref': 'envelope', 'data_name': f'journey_run_{run}'}\n",
    "        }\n",
    "        # text contains keys: 'offset_time', 'onset_time', 'sentence_boundaries', 'wordVec'\n",
    "        text_20000 = sio.loadmat(text_20000_dir / text_20000_files[run])\n",
    "        wordvec_20000_data = np.array(text_20000['wordVec'].tolist()).flatten()\n",
    "        wordvec_20000 = xr.Dataset(\n",
    "            {\n",
    "                'wordVec': (['time'], wordvec_20000_data),\n",
    "                'onset_time': (['time'], text_20000['onset_time'].flatten()),\n",
    "                'offset_time': (['time'], text_20000['offset_time'].flatten())\n",
    "            },\n",
    "\n",
    "        )\n",
    "        text_journey = sio.loadmat(text_journey_dir / text_journey_files[run])\n",
    "        wordvec_journey_data = np.array(text_journey['wordVec'].tolist()).flatten()\n",
    "        wordvec_journey = xr.Dataset(\n",
    "            {\n",
    "                'wordVec': (['time'], wordvec_journey_data),\n",
    "                'onset_time': (['time'], text_journey['onset_time'].flatten()),\n",
    "                'offset_time': (['time'], text_journey['offset_time'].flatten())\n",
    "            },\n",
    "\n",
    "        )\n",
    "        \n",
    "        # envelopes contains keys: 'envelope', 'fsEnv', 'origLength'\n",
    "        envelopes_20000 = sio.loadmat(envelopes_20000_dir / envelopes_20000_files[run])\n",
    "        envelopes_journey = sio.loadmat(envelopes_journey_dir / envelopes_journey_files[run])\n",
    "        \n",
    "        left_wordvec = xr.DataArray(\n",
    "            wordvec_20000['wordVec'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'wordvec',\n",
    "                'data_name': f'20000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_wordvec = xr.DataArray(\n",
    "            wordvec_journey['wordVec'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'wordvec',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'story': 'Journey to the Center of the Earth',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_onset_time = xr.DataArray(\n",
    "            wordvec_20000['onset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'onset_times',\n",
    "                'data_name': f'20000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_onset_time = xr.DataArray(\n",
    "            wordvec_journey['onset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'onset_times',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'story': 'Journey to the Center of the Earth',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_offset_time = xr.DataArray(\n",
    "            wordvec_20000['offset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'offset_times',\n",
    "                'data_name': f'20000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_offset_time = xr.DataArray(\n",
    "            wordvec_journey['offset_time'],\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'offset_times',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'story': 'Journey to the Center of the Earth',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_sentence_boundaries = xr.DataArray(\n",
    "            text_20000['sentence_boundaries'].flatten(),\n",
    "            dims = ['sentence_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'sentence_boundaries',\n",
    "                'data_name': f'20000_run_{run}',\n",
    "                'story': 'Twenty Thousand Leagues Under the Sea',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_sentence_boundaries = xr.DataArray(\n",
    "            text_journey['sentence_boundaries'].flatten(),\n",
    "            dims = ['sentence_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'sentence_boundaries',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        left_envelope = xr.DataArray(\n",
    "            envelopes_20000['envelope'].flatten(),\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'envelope',\n",
    "                'data_name': f'20000_run_{run}',\n",
    "                'sampling_frequency': int(envelopes_20000['fsEnv'][0][0]),\n",
    "                'original_length': int(envelopes_20000['origLength'][0][0]),\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        right_envelope = xr.DataArray(\n",
    "            envelopes_journey['envelope'].flatten(),\n",
    "            dims = ['word_number'],\n",
    "            attrs={\n",
    "                'schema_ref': 'envelope',\n",
    "                'data_name': f'journey_run_{run}',\n",
    "                'sampling_frequency': int(envelopes_journey['fsEnv'][0][0]),\n",
    "                'original_length': int(envelopes_journey['origLength'][0][0]),\n",
    "                'stimuli_record_data_ref': {'schema_ref': 'stimuli_record', 'data_name': f'session_{run}'}\n",
    "            }\n",
    "        )\n",
    "        yield left_wordvec, right_wordvec, left_onset_time, right_onset_time, left_offset_time, right_offset_time, left_sentence_boundaries, right_sentence_boundaries, left_envelope, right_envelope, stimuli_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T01:59:52.824532800Z",
     "start_time": "2024-07-03T01:59:52.608529400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_metamodel\n",
      "xarray_dataarray_metamodel\n",
      "version_timestamp\n",
      "schema_ref\n",
      "schema_type\n",
      "schema_name\n",
      "schema_title\n",
      "schema_description\n",
      "data_name\n",
      "time_of_save\n",
      "time_of_removal\n",
      "record_type\n",
      "json_schema\n",
      "has_file\n",
      "unit_of_measure\n",
      "dimension_of_measure\n",
      "acquisition\n",
      "acquisition_date\n",
      "import_date\n",
      "acquisition_notes\n",
      "data_dimensions\n",
      "shape\n",
      "dtype\n",
      "session_description\n",
      "session_date\n",
      "session_time\n",
      "session_duration\n",
      "session_notes\n",
      "data_ref\n",
      "start_time\n",
      "duration\n",
      "duration_unit\n",
      "animal_species\n",
      "age\n",
      "age_unit\n",
      "age_lower_bound\n",
      "age_upper_bound\n",
      "animal_id\n",
      "tetrode_id\n",
      "tetrode_depth\n",
      "genotype\n",
      "animal_strain\n",
      "stimulus_type\n",
      "stimulus_id\n",
      "stimulus_description\n",
      "recording_length\n",
      "sample_rate\n",
      "arena_shape\n",
      "arena_description\n",
      "study_description\n",
      "arena_height\n",
      "arena_width\n",
      "diameter\n",
      "arena_side_length\n",
      "arena_radius\n",
      "spike_count\n",
      "subject\n",
      "sampling_frequency\n",
      "attending_direction\n",
      "attending_story\n",
      "attend_score\n",
      "nonattend_score\n",
      "original_length\n",
      "story\n",
      "eeg_signal\n",
      "session\n",
      "stimuli_record\n",
      "wordvec\n",
      "offset_times\n",
      "onset_times\n",
      "sentence_boundaries\n",
      "envelope\n"
     ]
    }
   ],
   "source": [
    "filesystem = LocalFileSystem()\n",
    "# tmp_dir = TemporaryDirectory()\n",
    "# print(tmp_dir.name)\n",
    "\n",
    "# Create data storage location\n",
    "dataset_name = \"cocktail_party\"\n",
    "store_path = Path(\"D:\\BlcRepo\\OtherCode\\Generative_Neuroscience\\Temp_Dataset\")\n",
    "store_path = store_path / dataset_name\n",
    "\n",
    "# Create a directory for the dataset\n",
    "if not os.path.exists(store_path):\n",
    "    os.makedirs(store_path)\n",
    "\n",
    "tmp_dir_fs = DirFileSystem(\n",
    "    store_path,\n",
    "    filesystem=filesystem\n",
    ")\n",
    "client = MongoClient()\n",
    "memory_store = {}\n",
    "uow_provider = UnitOfWorkProvider(\n",
    "    mongo_client=client,\n",
    "    filesystem=tmp_dir_fs,\n",
    "    memory_store=memory_store\n",
    ")\n",
    "import json\n",
    "cwd = Path.cwd()\n",
    "domain_models_path = cwd.parent / \"DomainModels\\\\cocktail_party\\\\data_models.json\"\n",
    "metamodel_path = cwd.parent / \"DomainModels\\\\cocktail_party\\\\metamodels.json\"\n",
    "property_path = cwd.parent / \"DomainModels\\\\cocktail_party\\\\property_models.json\"\n",
    "\n",
    "with open(metamodel_path) as f:\n",
    "    metamodels = json.load(f)\n",
    "\n",
    "with open(property_path) as f:\n",
    "    property_models = json.load(f)\n",
    "    \n",
    "# for metamodel in metamodels:\n",
    "# with uow_provider('cocktail-party') as uow:\n",
    "#     print(f\"Adding model {metamodel['schema_name']} to domain_models store.\")\n",
    "#     uow.domain_models.add(metamodel)\n",
    "#     model = uow.domain_models.get(metamodel['schema_name'])\n",
    "#     print(model['schema_name'])\n",
    "#     uow.commit()\n",
    "\n",
    "# load domain models json file\n",
    "with open(domain_models_path) as f:\n",
    "    domain_models = json.load(f)\n",
    "    \n",
    "with uow_provider('cocktail-party') as uow:\n",
    "    for metamodel in metamodels:\n",
    "        uow.domain_models.add(metamodel)\n",
    "        model = uow.domain_models.get(metamodel['schema_name'])\n",
    "        print(model['schema_name'])\n",
    "    for property_model in property_models:\n",
    "        uow.domain_models.add(property_model)\n",
    "        model = uow.domain_models.get(property_model['schema_name'])\n",
    "        print(model['schema_name'])\n",
    "    for domain_model in domain_models:\n",
    "        uow.domain_models.add(domain_model)\n",
    "        model = uow.domain_models.get(domain_model['schema_name'])\n",
    "        print(model['schema_name'])\n",
    "        uow.commit()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'time': length 2 on the data but length 128 on coordinate 'time'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m eeg_xarray, mastoid_xarray \u001B[38;5;129;01min\u001B[39;00m load_eeg_data(data_path):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m uow_provider(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcocktail-party\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m uow:\n\u001B[0;32m      3\u001B[0m         uow\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39madd(eeg_xarray)\n",
      "Cell \u001B[1;32mIn[3], line 46\u001B[0m, in \u001B[0;36mload_eeg_data\u001B[1;34m(data_path)\u001B[0m\n\u001B[0;32m     27\u001B[0m behavior_score \u001B[38;5;241m=\u001B[39m behavior_scores[sub_n\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, run_n\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     28\u001B[0m eeg_xarray \u001B[38;5;241m=\u001B[39m xr\u001B[38;5;241m.\u001B[39mDataArray(\n\u001B[0;32m     29\u001B[0m     data\u001B[38;5;241m=\u001B[39meeg_data,\n\u001B[0;32m     30\u001B[0m     dims\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchannel\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     44\u001B[0m     }\n\u001B[0;32m     45\u001B[0m )\n\u001B[1;32m---> 46\u001B[0m mastoid_xarray \u001B[38;5;241m=\u001B[39m \u001B[43mxr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataArray\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmastoid_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mchannel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoords\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mchannel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mleft\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mright\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mschema_ref\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meeg_signal\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata_name\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msubject_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43msub_n\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_run_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mrun_n\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_mastoid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msubject\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msubject_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43msub_n\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msession_data_ref\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mschema_ref\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msession\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata_name\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msession_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mrun_n\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msampling_frequency\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattending_direction\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mattending_direction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattending_story\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mattending_story\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattend_score\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbehavior_score\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnonattend_score\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbehavior_score\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m eeg_xarray, mastoid_xarray\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SQL_py_read\\lib\\site-packages\\xarray\\core\\dataarray.py:456\u001B[0m, in \u001B[0;36mDataArray.__init__\u001B[1;34m(self, data, coords, dims, name, attrs, indexes, fastpath)\u001B[0m\n\u001B[0;32m    454\u001B[0m data \u001B[38;5;241m=\u001B[39m _check_data_shape(data, coords, dims)\n\u001B[0;32m    455\u001B[0m data \u001B[38;5;241m=\u001B[39m as_compatible_data(data)\n\u001B[1;32m--> 456\u001B[0m coords, dims \u001B[38;5;241m=\u001B[39m \u001B[43m_infer_coords_and_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcoords\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdims\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    457\u001B[0m variable \u001B[38;5;241m=\u001B[39m Variable(dims, data, attrs, fastpath\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(coords, Coordinates):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SQL_py_read\\lib\\site-packages\\xarray\\core\\dataarray.py:195\u001B[0m, in \u001B[0;36m_infer_coords_and_dims\u001B[1;34m(shape, coords, dims)\u001B[0m\n\u001B[0;32m    192\u001B[0m             var\u001B[38;5;241m.\u001B[39mdims \u001B[38;5;241m=\u001B[39m (dim,)\n\u001B[0;32m    193\u001B[0m             new_coords[dim] \u001B[38;5;241m=\u001B[39m var\u001B[38;5;241m.\u001B[39mto_index_variable()\n\u001B[1;32m--> 195\u001B[0m \u001B[43m_check_coords_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_coords\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdims_tuple\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m new_coords, dims_tuple\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SQL_py_read\\lib\\site-packages\\xarray\\core\\dataarray.py:129\u001B[0m, in \u001B[0;36m_check_coords_dims\u001B[1;34m(shape, coords, dim)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d, s \u001B[38;5;129;01min\u001B[39;00m v\u001B[38;5;241m.\u001B[39msizes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m s \u001B[38;5;241m!=\u001B[39m sizes[d]:\n\u001B[1;32m--> 129\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    130\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconflicting sizes for dimension \u001B[39m\u001B[38;5;132;01m{\u001B[39;00md\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    131\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msizes[d]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m on the data but length \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m on \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    132\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoordinate \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    133\u001B[0m         )\n",
      "\u001B[1;31mValueError\u001B[0m: conflicting sizes for dimension 'time': length 2 on the data but length 128 on coordinate 'time'"
     ]
    }
   ],
   "source": [
    "for eeg_xarray, mastoid_xarray in load_eeg_data(data_path):\n",
    "    with uow_provider('cocktail-party') as uow:\n",
    "        uow.data.add(eeg_xarray)\n",
    "        uow.data.add(mastoid_xarray)\n",
    "        \n",
    "        uow.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T02:02:02.139646400Z",
     "start_time": "2024-07-03T02:00:14.882088900Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for result in load_stimuli(data_path):\n",
    "    for data in result:\n",
    "        with uow_provider('cocktail-party') as uow:\n",
    "            uow.data.add(data)\n",
    "            uow.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T02:00:05.388488500Z",
     "start_time": "2024-07-03T01:59:59.452802900Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envelopes_path = \"D:\\BlcRepo\\OtherCode\\Generative_Neuroscience\\Dataset\\Broderick\\Cocktail Party\\Stimuli\\Envelopes\\\\20000\\\\20000_1_env.mat\"\n",
    "envelopes_path = os.path.join(data_path, \"Stimuli\", \"Envelopes\", \"20000\", \"20000_1_env.mat\")\n",
    "matf = sio.loadmat(envelopes_path)\n",
    "print(matf.keys())\n",
    "print(matf['envelope'].shape)\n",
    "print(matf['fsEnv'])\n",
    "print(matf['origLength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_path = \"K:\\ke\\sta\\data\\SpeechEEG\\Cocktail Party\\Stimuli\\Text\\\\20000\\Run4.mat\"\n",
    "text_path = os.path.join(data_path, \"Stimuli\", \"Text\", \"20000\", \"Run4.mat\")\n",
    "matf = sio.loadmat(text_path)\n",
    "print(matf.keys())\n",
    "def utf_array_to_str(arr):\n",
    "    string = \"\"\n",
    "    for word in arr:\n",
    "        string += word[0][0] + \" \"\n",
    "    return string\n",
    "print(utf_array_to_str(matf['wordVec']))\n",
    "print(matf['wordVec'].shape)\n",
    "print(matf['onset_time'].shape)\n",
    "print(matf['offset_time'].shape)\n",
    "print(matf['sentence_boundaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def compute_unique_vocabulary(text_dir):\n",
    "    full_paths = [text_dir / \"20000\" / f for f in os.listdir(text_dir / \"20000\")] \\\n",
    "                + [text_dir / \"Journey\" / f for f in os.listdir(text_dir / \"Journey\")]\n",
    "    word_counts = {}\n",
    "    word_lookup = {}\n",
    "    for text_file in full_paths:\n",
    "        matf = sio.loadmat(text_file)\n",
    "        run = text_file.stem.replace(\"Run\", \"\")\n",
    "        source = text_file.parent.stem\n",
    "        for word in matf['wordVec']:\n",
    "            for word in word:\n",
    "                word = str(word[0])\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                    word_lookup[word] += [(source, f'session_{run}')]\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "                    word_lookup[word] = [(source, f'session_{run}')]\n",
    "    return word_counts, word_lookup\n",
    "vocab, lookup = compute_unique_vocabulary(data_path / \"Stimuli\" / \"Text\")\n",
    "print(len(vocab))\n",
    "print(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
